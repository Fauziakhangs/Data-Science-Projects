---
title: "HW4"
output: pdf_document
date: "2023-05-21"
---
## Problem 1
### For this problem, you will tune and apply kNN and compare it to other classifiers. We will use the wine quality data, which has a number of measurements about chemical components in wine, plus a quality rating. There are separate files for red and white wines, so the first step is some data preparation.

### a. Load the two provided wine quality datasets and prepare them by (1) ensuring that all the variables have the right type (e.g., what is numeric vs. factor), (2) adding a type column to each that indicates if it is red or white wine and (2) merging the two tables together into one table (hint: try full_join()). You now have one table that contains the data on red and white wine, with a column that tells if the wine was from the red or white set (the type column you made).
```{r }
library(dplyr)

getwd()
mydata1 <- read.csv("C:/Users/HP/Downloads/winequality-white.csv", sep = ";")
mydt1<- data.frame (mydata1)
#View(mydt1)
head(mydt1)
str(mydt1)

mydata2<- read.csv("C:/Users/HP/Downloads/winequality-red.csv", sep = ";")
mydt2<- data.frame (mydata2)
mydata2

mydt1$type <- "white"
mydt2$type <- "red"

mydt1 <- type.convert(mydt1, as.is = TRUE)
mydt2 <- type.convert(mydt2, as.is = TRUE)

wine_data <- full_join(mydt1, mydt2)
head(wine_data)
View(wine_data)
```

### b. Use PCA to create a projection of the data to 2D and show a scatterplot with color showing the wine type. 

```{r}
library(caret)
library(ggplot2)
sum(is.na(wine_data))
head(wine_data)
wine_data$type <- as.factor(wine_data$type)
wine_dummy <- dummyVars(type ~., data = wine_data)

wine_data_encoded <- as.data.frame(predict(wine_dummy, newdata = wine_data))
head(wine_data_encoded)
wine_dt.pca <- prcomp(wine_data_encoded)
summary(wine_dt.pca)

wine_pca  <- as.data.frame(wine_dt.pca$x[, 1:2])
wine_pca$type <- wine_data$type

ggplot(wine_pca, aes(x = PC1, y = PC2, color = type)) +
  geom_point() +
  labs(x = "PC1", y = "PC2", color = "Wine Type") +
  theme_minimal()


```

### c. We are going to try kNN, SVM and decision trees on this data. Based on the ‘shape’ of the data in the visualization from (b), which do you think will do best and why? You can also embed plots, for example:

```{r}
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)
knnFit <- train(type ~., data = wine_data, method = "knn",
                trControl = ctrl,
                preProcess = c("center", "scale"))
knnFit
svm1 <- train(type ~., data = wine_data, method= "svmLinear")
svm1

train_control = trainControl(method = "cv", number = 10)
tree1 <- train(type~., data = wine_data, method = "rpart", trControl = train_control)
tree1
```

### d. Use kNN (tune k), use decision trees (basic rpart method is fine), and SVM (tune C) to predict type from the rest of the variables. Compare the accuracy values – is this what you expected? Can you explain it?
Note: you will need to fix the columns names for rpart because it is not able to handle the underscores. This code will do the trick (assuming you called your data wine_quality):
 colnames(wine_quality) <- make.names(colnames(wine_quality))

```{r }
set.seed(123)
ctrl <- trainControl(method="cv", number = 10)
knnFit <- train(type ~., data = wine_data, method = "knn", 
                trControl = ctrl, preProcess = c("center", "scale"),
                tuneLength = 15)

plot(knnFit)
#k=7 has the highest accuracy
train_control = trainControl(method = "cv", number = 10)
tree1 <- train(type ~., data = wine_data, method = "rpart", trControl = train_control)
tree1

#SVM
grid <- expand.grid(C = 10^seq(-5,3,0.7))
#fit the model
svm_grid <- train(type ~., data = wine_data, method = "svmLinear",
                  trControl = train_control, tuneGrid = grid)

svm_grid
# The highest accuracy of 0.9949219 was achieved when C was set to 0.7943282. Therefore, the best value of C for this SVM model is 0.7943282.


```

### e. Use the same already computed PCA again to show a scatter plot of the data and to visualize the labels for kNN, decision tree and SVM. Note that you do not need to recreate the PCA projection, you have already done this in 1b. Here, you just make a new visualization for each classifier using its labels for color (same points but change the color). Map the color results to the classifier, that is use the “predict” function to predict the class of your data, add it to your data frame and use it as a color. This is done for KNN in the tutorial, it should be similar for the others. Consider and explain the differences in how these classifiers performed.

```{r }

knn_pred <- predict(knnFit, newdata = wine_data_encoded)
wine_pca$kNN_label <- knn_pred

tree_pred <- predict(tree1, newdata = wine_data)
wine_pca$Tree_label <- tree_pred

svm_pred <- predict(svm1, newdata = wine_data)
wine_pca$SVM_label <- svm_pred

ggplot(wine_pca, aes(x = PC1, y = PC2, color = kNN_label)) +
  geom_point() +
  labs(x = "PC1", y = "PC2", color = "kNN Label") +
  theme_minimal()

ggplot(wine_pca, aes(x = PC1, y = PC2, color = Tree_label)) +
  geom_point() +
  labs(x = "PC1", y = "PC2", color = "Decision Tree Label") +
  theme_minimal()

ggplot(wine_pca, aes(x = PC1, y = PC2, color = SVM_label)) +
  geom_point() +
  labs(x = "PC1", y = "PC2", color = "SVM Label") +
  theme_minimal()

```

## Problem 2
### In this question we will use the Sacramento data, which covers available housing in the region of that city.The variables include numerical information about the size of the housing and its price, as well as categorical information like zip code (there are a large but limited number in the area), and the type of unit (condo vs house (coded as residential)).


### a. Load the data from the tidyverse library with the data(“Sacramento”) command and you should have a variable Sacramento. Because we have categoricals, convert them to dummy variables.

```{r }
library(tidyverse)
library(caret)
data("Sacramento")
sum(is.na(Sacramento$type))
Sacramento$type <- as.factor(Sacramento$type)
head(Sacramento$type)
dummy1 <- dummyVars(type ~., data = Sacramento)
dummies1 <- as.data.frame(predict(dummy1, newdata = Sacramento))
head(dummies1)
```

### b. With kNN, because of the high dimensionality, which might be a good choice for the distance function?

```{r }
#Without applying different metrics it is hard to tell. Although, Minkowski is the commonly used approach.
```

### c. Use kNN to classify this data with type as the label. Tune the choice of k plus the type of distance function. Report your results – what values for these parameters were tried, which were chosen, and how did they perform with accuracy?

```{r }
s_dummies1 <- dummies1
s_dummies1$type <- Sacramento$type
library(kknn)
zero_var_cols <- nearZeroVar(s_dummies1)

# Remove variables with zero variance
s_dummies1 <- s_dummies1[, -zero_var_cols]
tuneGrid <- expand.grid(kmax = 3:7,
                        kernel = c("rectangular", "cos"),
                        distance = 1:3)


kknn_fit <- train(type ~., 
                  data = s_dummies1,
                  method = 'kknn',
                  trControl = ctrl,
                  preProcess = c('center', 'scale'),
                                 tuneGrid = tuneGrid)
kknn_fit


```
Using Minkowski distance from 1 to 3 and generated regular tests and cosine distance functions. Overall, kmax of 6 with a distance 1 and consine based distance is found to be the resulted value because it performed better and the accuracy can be comparable.


## Problem 3
### In this problem we will continue with the wine quality data from Problem 1, but this time we will use clustering. Do not forget to remove the type variable before clustering because that would be cheating by using the label to perform clustering.
### a. Use k-means to cluster the data. Show your usage of silhouette and the elbow method to pick the best number of clusters. Make sure it is using multiple restarts.

```{r }
library(kknn)
library(factoextra)
getwd()
mydata1 <- read.csv("C:/Users/HP/Downloads/winequality-white.csv", sep = ";")
mydt1<- data.frame (mydata1)
#View(mydt1)
head(mydt1)
str(mydt1)

mydata2<- read.csv("C:/Users/HP/Downloads/winequality-red.csv", sep = ";")
mydt2<- data.frame (mydata2)
mydata2

mydt1$type <- "white"
mydt2$type <- "red"

mydt1 <- type.convert(mydt1, as.is = TRUE)
mydt2 <- type.convert(mydt2, as.is = TRUE)

wine_data1 <- full_join(mydt1, mydt2)
head(wine_data1)
length(wine_data1$type)

wine_data <- wine_data1 %>% select(-c("type"))
df <- wine_data
head(df)

preproc <- preProcess(df, method = c("center", "scale"))
predictors <- predict(preproc, df)

fviz_nbclust(predictors, kmeans, method = "wss")
fviz_nbclust(predictors, kmeans , method = "silhouette")

#fitting the data
fit <- kmeans(predictors, centers = 4, nstart = 25)
fit

```

### b. Use hierarchical agglomerative clustering (HAC) to cluster the data. Try at least 2 distance functions and at least 2 linkage functions (cluster distance functions), for a total of 4 parameter combinations. For each parameter combination, perform the clustering.

```{r }
dist_mist <- dist(predictors, method = 'euclidean')
hfit <- hclust(dist_mist, method = 'complete')
plot(hfit)
h1 <- cutree(hfit, k = 4)
length(h1)
hfit1 <- hclust(dist_mist, method = 'average')
plot(hfit1)
h2 <- cutree(hfit1, k = 4)

dist_mst <- dist(predictors, method = 'manhattan')
hfit2 <-hclust(dist_mst, method = 'complete')
plot(hfit2)
h3 <- cutree(hfit2, k = 4)

hfit3 <- hclust(dist_mst, method = 'average')
plot(hfit3)
h4 <- cutree(hfit3, k = 4)

```

### c. Compare the k-means and HAC clusterings by creating a crosstabulation between their labels.

```{r }
result1 <- data.frame(Type = wine_data1$type, HCA1 = h1, kmeans = fit$cluster)
length(wine_data$type)
length(h1)
length(fit$cluster)

result2 <- data.frame(Type = wine_data1$type, HCA2 = h2, kmeans = fit$cluster)
result3 <- data.frame(Type = wine_data1$type, HCA3 = h3, kmeans = fit$cluster)
result4 <- data.frame(Type = wine_data1$type, HCA4 = h4, kmeans = fit$cluster)

#cross Tab HAC

result1 %>%
  group_by(HCA1) %>%
  select(HCA1, Type) %>%
  table()


result2 %>%
  group_by(HCA2) %>%
  select(HCA2, Type) %>%
  table()

result3 %>%
  group_by(HCA3) %>%
  select(HCA3, Type) %>%
  table()

result4 %>%
  group_by(HCA4) %>%
  select(HCA4, Type) %>%
  table()



result1 %>%
  group_by(kmeans) %>%
  select(kmeans, Type) %>%
  table()

result2 %>%
  group_by(kmeans) %>%
  select(kmeans, Type) %>%
  table()

result3 %>%
  group_by(kmeans) %>%
  select(kmeans, Type) %>%
  table()

result4 %>%
  group_by(kmeans) %>%
  select(kmeans, Type) %>%
  table()
```

### d. For comparison – use PCA to visualize the data in a scatterplot. Create 3 separate plots: use the color of the points to show (1) the type label, (2) the k-means cluster labels and (3) the HAC cluster labels.


```{r }
library(ggplot2)
set.seed(123)
pca <- prcomp(wine_data_encoded)
rotated_data <- as.data.frame(pca$x)
names(wine_data1)
rotated_data$Color <- wine_data1$type
names(rotated_data)

ggplot(rotated_data, aes(x = PC1, y = PC2, color = Color)) +
  geom_point() +
  labs(x = "PC1", y = "PC2", color = "Wine Type") +
  theme_minimal()

rotated_data$Clusters = as.factor(h3)
ggplot(data = rotated_data, aes(x = PC1, y= PC2, col = Clusters)) + geom_point()

rotated_data$Clusters = as.factor(fit$cluster)
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Clusters)) + geom_point()

```

### e. Consider the results of C and D and explain the differences between the clustering results in terms of how the algorithms work

The clustering methods are less randomized when using predetermined number of clusters (k) in k means. However, HAC does not require specifying the number of clusters in advance. 
```{r }

```

## Problem 4
### Back to the Starwars data from a previous assignment! Remember that the variable that lists the actual names and the variables that are actually lists will be a problem, so remove them (name, films, vehicles, starships). Make sure to double check the types of the variables, i.e., that they are numerical or factors as you expect.
```{r }
data("starwars")
dt_starwars <- starwars
dt_starwars <- dt_starwars %>% select(-c("name", "films", "vehicles","starships"))
dt_starwars <- na.omit(dt_starwars)
sum(is.na(dt_starwars))
```
### a. Use hierarchical agglomerative clustering to cluster the Starwars data. This time we can leave the categorical variables in place, because we will use the gower metric from daisy in the cluster library to get the distances. Use average linkage. Determine the best number of clusters.
```{r }
library(cluster)
library(kknn)
library(factoextra)
#dataset contains both numerical and categorical data
dt_starwars[, c(3, 4, 5, 7, 8, 9, 10)] <- lapply(dt_starwars[, c(3, 4, 5, 7, 8, 9, 10)], factor)

dist_mat <- daisy(dt_starwars, metric = "gower")
preproc <- preProcess(dt_starwars, method = "center", "scale")
predictors <- predict(preproc, dt_starwars)
fviz_nbclust(predictors, FUNcluster = hcut, method = "silhouette")

#using average method
# k value is 2
hfit <- hclust(dist_mat, method = 'average')
h2 <- cutree(hfit, k = 2)
summary(h2)

```

### b. Produce the dendogram for (a). How might an anomaly show up in a dendogram? Do you see a Starwars character who does not seem to fit in easily? What is the advantage of considering anomalies this way as opposed to looking for unusual values relative to the mean and standard deviations, as we considered earlier in the course? Disadvantages?
```{r }
hfit <- hclust(dist_mat, method = 'average')
plot(hfit)
# In the Starwars dataset,Yoda may appear as an outlier or separate branch in the dendrogram.
# The advantage of considering anomalies in the 
# dendrogram is that it provides a visual representation 
# of the relationships and similarities among data points. 
# It allows us to identify unusual patterns or outliers
# Anomalies based on heights are reviewed however, other features must 
# also be considered like eye color etc.

```

### c. Use dummy variables to make this data fully numeric and then use k-means to cluster. Choose the best number of clusters.

```{r }

dummy <- dummyVars(gender ~., data = dt_starwars)
dummies <- as.data.frame(predict(dummy, newdata = dt_starwars))
head(dummies)

predictors <- dummies

set.seed(123)
preproc <- preProcess(predictors, method = c("center", "scale"))
predictors <- predict(preproc, predictors)
fviz_nbclust(predictors, kmeans, method = "wss")
fviz_nbclust(predictors, kmeans, method = "silhouette")
#k = 2
#fit the data
fit <- kmeans(predictors, centers = 2, nstart = 25)
fit

```

### d. Compare the HAC and k-means clusterings with a crosstabulation.

```{r }
#Cross tabulation for HAC and kmeans 
result <- data.frame(Gender = dt_starwars$gender, HAC2 =h2, kmeans = fit$cluster)
result %>% group_by(HAC2) %>% select(HAC2, Gender) %>% table()
result %>% group_by(kmeans) %>% select(kmeans, Gender) %>% table()
```


